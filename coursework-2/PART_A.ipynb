{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_NLP_PART_A_Week_8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA8Z1wm5lW4W",
        "colab_type": "code",
        "outputId": "ef5727b4-7f9b-4e16-f1c8-df98a02f0349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Embedding,LSTM,Dropout,Dense,Layer\n",
        "from keras import Model,Input\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "import collections\n",
        "import numpy as np\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "class NmtModel(object):\n",
        "  def __init__(self,source_dict,target_dict,use_attention):\n",
        "    self.hidden_size = 200\n",
        "    self.embedding_size = 100\n",
        "    self.hidden_dropout_rate=0.2\n",
        "    self.embedding_dropout_rate = 0.2\n",
        "    self.batch_size = 100\n",
        "    self.max_target_step = 30\n",
        "    self.vocab_target_size = len(target_dict.vocab)\n",
        "    self.vocab_source_size = len(source_dict.vocab)\n",
        "    self.target_dict = target_dict\n",
        "    self.source_dict = source_dict\n",
        "    self.SOS = target_dict.word2ids['<start>']\n",
        "    self.EOS = target_dict.word2ids['<end>']\n",
        "    self.use_attention = use_attention\n",
        "\n",
        "    print(\"source vocab: %d, target vocab:%d\" % (self.vocab_source_size,self.vocab_target_size))\n",
        "\n",
        "\n",
        "  def build(self):\n",
        "    source_words = Input(shape=(None,),dtype='int32')\n",
        "    target_words = Input(shape=(None,), dtype='int32')\n",
        "\n",
        "    \"\"\"\n",
        "    Task 1 encoder\n",
        "    \n",
        "    Start\n",
        "    \"\"\"\n",
        "    # implementing embedding layer and encoder\n",
        "\n",
        "    # source embedding layer with input dim as source vocab size and output as embedding size\n",
        "    # these are trainable layer with mask_zero = true to get rid of the padding\n",
        "    embedding_source = Embedding(self.vocab_source_size, self.embedding_size, trainable=True, mask_zero = True)\n",
        "    # target embedding layer with input dim as target vocab size and output as embedding size\n",
        "    embedding_target = Embedding(self.vocab_target_size, self.embedding_size, trainable=True, mask_zero = True)\n",
        "    # passing source words through the source embedding layer and adding dropout\n",
        "    source_words_embeddings = embedding_source(source_words)\n",
        "    source_words_embeddings = Dropout(self.embedding_dropout_rate)(source_words_embeddings)\n",
        "    # passing target words through the target embedding layer and adding dropout\n",
        "    target_words_embeddings = embedding_target(target_words)\n",
        "    target_words_embeddings = Dropout(self.embedding_dropout_rate)(target_words_embeddings)\n",
        "    # LSTM layer with units = self.hidden_size\n",
        "    # return sequence is ture to get the output of all tokens\n",
        "    # return state is true to get the hidden state and cell state from the encoder\n",
        "    encoder_lstm = LSTM(self.hidden_size, recurrent_dropout=self.hidden_dropout_rate, return_sequences=True, return_state=True)\n",
        "    # passing the source word embeddings to encoder_lstm\n",
        "    encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(source_words_embeddings)\n",
        "    \n",
        "    \"\"\"\n",
        "    End Task 1\n",
        "    \"\"\"\n",
        "    encoder_states = [encoder_state_h,encoder_state_c]\n",
        "\n",
        "    decoder_lstm = LSTM(self.hidden_size,recurrent_dropout=self.hidden_dropout_rate,return_sequences=True,return_state=True)\n",
        "    decoder_outputs_train,_,_ = decoder_lstm(target_words_embeddings,initial_state=encoder_states)\n",
        "\n",
        "\n",
        "    if self.use_attention:\n",
        "      decoder_attention = AttentionLayer()\n",
        "      decoder_outputs_train = decoder_attention([encoder_outputs,decoder_outputs_train])\n",
        "\n",
        "    decoder_dense = Dense(self.vocab_target_size,activation='softmax')\n",
        "    decoder_outputs_train = decoder_dense(decoder_outputs_train)\n",
        "\n",
        "    adam = Adam(lr=0.01,clipnorm=5.0)\n",
        "    self.train_model = Model([source_words,target_words], decoder_outputs_train)\n",
        "    self.train_model.compile(optimizer=adam,loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    self.train_model.summary()\n",
        "\n",
        "    #Inference Models\n",
        "\n",
        "    self.encoder_model = Model(source_words,[encoder_outputs,encoder_state_h,encoder_state_c])\n",
        "    self.encoder_model.summary()\n",
        "\n",
        "    decoder_state_input_h = Input(shape=(self.hidden_size,))\n",
        "    decoder_state_input_c = Input(shape=(self.hidden_size,))\n",
        "    encoder_outputs_input = Input(shape=(None,self.hidden_size,))\n",
        "\n",
        "    \"\"\"\n",
        "    Task 2 decoder for inference\n",
        "    \n",
        "    Start\n",
        "    \"\"\"\n",
        "    # putting decoder state input h and state input c in a list\n",
        "    decoder_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "    # passing targer_words_embeddings through the decoder lstm\n",
        "    # passing decoder states as the initial state for the decoder lstm\n",
        "    decoder_outputs_test, decoder_state_output_h, decoder_state_output_c = decoder_lstm(target_words_embeddings,initial_state=decoder_states)\n",
        "    # check if attention layer should be used\n",
        "    # attention layer is used to provide the decoder with information from every encoder hidden state\n",
        "    if self.use_attention:\n",
        "      decoder_attention = AttentionLayer()\n",
        "      decoder_outputs_test = decoder_attention([encoder_outputs_input,decoder_outputs_test])\n",
        "    # passing output of the decoder lstm (attention = false) or decoder_attention (attention = true) to the decoder dense layer\n",
        "    decoder_outputs_test = decoder_dense(decoder_outputs_test)\n",
        "    \n",
        "    \"\"\"\n",
        "    End Task 2 \n",
        "    \"\"\"\n",
        "\n",
        "    self.decoder_model = Model([target_words,decoder_state_input_h,decoder_state_input_c,encoder_outputs_input],\n",
        "                               [decoder_outputs_test,decoder_state_output_h,decoder_state_output_c])\n",
        "    self.decoder_model.summary()\n",
        "\n",
        "\n",
        "\n",
        "  def time_used(self, start_time):\n",
        "    curr_time = time.time()\n",
        "    used_time = curr_time-start_time\n",
        "    m = used_time // 60\n",
        "    s = used_time - 60 * m\n",
        "    return \"%d m %d s\" % (m, s)\n",
        "\n",
        "  def train(self,train_data,dev_data,test_data, epochs):\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      print(\"Starting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "      epoch_time = time.time()\n",
        "      source_words_train, target_words_train, target_words_train_labels = train_data\n",
        "\n",
        "      self.train_model.fit([source_words_train,target_words_train],target_words_train_labels,batch_size=self.batch_size)\n",
        "\n",
        "      print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "      dev_time = time.time()\n",
        "      print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "      self.eval(dev_data)\n",
        "      print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "    print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "    print(\"Evaluating on test set:\")\n",
        "    test_time = time.time()\n",
        "    self.eval(test_data)\n",
        "    print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "\n",
        "\n",
        "  def get_target_sentences(self, sents,vocab,reference=False):\n",
        "    str_sents = []\n",
        "    num_sent, max_len = sents.shape\n",
        "    for i in range(num_sent):\n",
        "      str_sent = []\n",
        "      for j in range(max_len):\n",
        "        t = sents[i,j].item()\n",
        "        if t == self.SOS:\n",
        "          continue\n",
        "        if t == self.EOS:\n",
        "          break\n",
        "\n",
        "        str_sent.append(vocab[t])\n",
        "      if reference:\n",
        "        str_sents.append([str_sent])\n",
        "      else:\n",
        "        str_sents.append(str_sent)\n",
        "    return str_sents\n",
        "\n",
        "\n",
        "  def eval(self, dataset):\n",
        "    source_words, target_words_labels = dataset\n",
        "    vocab = self.target_dict.vocab\n",
        "\n",
        "    encoder_outputs, state_h,state_c = self.encoder_model.predict(source_words,batch_size=self.batch_size)\n",
        "    predictions = []\n",
        "    step_target_words = np.ones([source_words.shape[0],1]) * self.SOS\n",
        "    for _ in range(self.max_target_step):\n",
        "      step_decoder_outputs, state_h,state_c = self.decoder_model.predict([step_target_words,state_h,state_c,encoder_outputs],batch_size=self.batch_size)\n",
        "      step_target_words = np.argmax(step_decoder_outputs,axis=2)\n",
        "      predictions.append(step_target_words)\n",
        "\n",
        "    candidates = self.get_target_sentences(np.concatenate(predictions,axis=1),vocab)\n",
        "    references = self.get_target_sentences(target_words_labels,vocab,reference=True)\n",
        "    score = corpus_bleu(references,candidates)\n",
        "    print(\"Model BLEU score: %.2f\" % (score*100.0))\n",
        "\n",
        "    # code to print the first 5 sample outputs: predicted outputs are candidates and actual sentences are references\n",
        "    print(\"===== PRINTING OUTPUT====\")\n",
        "    for i in range(5):\n",
        "      print(\"{} Sample\".format(i+1))\n",
        "      print(\"Predicted Sentence: \"+\" \".join(candidates[i]))\n",
        "      print(\"Actual Sentence: \"+\" \".join(references[i][0]))\n",
        "    print(\"========================\")\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    if mask == None:\n",
        "      return None\n",
        "    return mask[1]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[1][0],input_shape[1][1],input_shape[1][2]*2)\n",
        "\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    encoder_outputs, decoder_outputs = inputs\n",
        "\n",
        "    \"\"\"\n",
        "    Task 3 attention\n",
        "    \n",
        "    Start\n",
        "    \"\"\"\n",
        "    # This attention mechanism computes the score between decoder_outpus and encoder_outputs by matrix multiplication\n",
        "    # to multiply decoder_outputs and encoder_outputs, it is needed to transpose the last two dimensions of decoder_outputs\n",
        "    # the last two dimensions are transposed as given below\n",
        "    decoder_outputs_dim = K.permute_dimensions(decoder_outputs, (0,2,1))\n",
        "    # luong_score is calculated by using a dot product  of encoder_outputs and decoder_outputs\n",
        "    luong_score = K.batch_dot(encoder_outputs, decoder_outputs_dim, axes=None)\n",
        "    # softmax is applied to get the attention score\n",
        "    luong_score = K.softmax(luong_score, axis=1)\n",
        "    # expand the dimensions so that encoder_outputs and luong score(attention score) has the same shape\n",
        "    encoder_outputs = K.expand_dims(encoder_outputs, axis=2)\n",
        "    luong_score = K.expand_dims(luong_score, axis=3)\n",
        "    # encoded vector is created by doing element wise multiplication between encoder_outputs and luong_score\n",
        "    encoder_vector = encoder_outputs * luong_score\n",
        "    # sum the ecoder_vector along the axis = 1 (max_source_sent_len) to get the required encoder_vector \n",
        "    encoder_vector = K.sum(encoder_vector, axis=1)\n",
        "    \n",
        "    \"\"\"\n",
        "    End Task 3\n",
        "    \"\"\"\n",
        "    # [batch,max_dec,2*emb]\n",
        "    new_decoder_outputs = K.concatenate([decoder_outputs, encoder_vector])\n",
        "\n",
        "    return new_decoder_outputs\n",
        "\n",
        "\n",
        "class LanguageDict():\n",
        "  def __init__(self, sents):\n",
        "    word_counter = collections.Counter(tok.lower() for sent in sents for tok in sent)\n",
        "\n",
        "    self.vocab = []\n",
        "    self.vocab.append('<pad>') #zero paddings\n",
        "    self.vocab.append('<unk>')\n",
        "    self.vocab.extend([t for t,c in word_counter.items() if c > 10])\n",
        "\n",
        "    self.word2ids = {w:id for id, w in enumerate(self.vocab)}\n",
        "    self.UNK = self.word2ids['<unk>']\n",
        "    self.PAD = self.word2ids['<pad>']\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset(source_path,target_path, max_num_examples=30000):\n",
        "  source_lines = open(source_path).readlines()\n",
        "  target_lines = open(target_path).readlines()\n",
        "  assert len(source_lines) == len(target_lines)\n",
        "  if max_num_examples > 0:\n",
        "    max_num_examples = min(len(source_lines), max_num_examples)\n",
        "    source_lines = source_lines[:max_num_examples]\n",
        "    target_lines = target_lines[:max_num_examples]\n",
        "\n",
        "  source_sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in source_lines]\n",
        "  target_sents = [[tok.lower() for tok in sent.strip().split(' ')] for sent in target_lines]\n",
        "  for sent in target_sents:\n",
        "    sent.append('<end>')\n",
        "    sent.insert(0,'<start>')\n",
        "\n",
        "  source_lang_dict = LanguageDict(source_sents)\n",
        "  target_lang_dict = LanguageDict(target_sents)\n",
        "\n",
        "  unit = len(source_sents)//10\n",
        "\n",
        "  source_words = [[source_lang_dict.word2ids.get(tok,source_lang_dict.UNK) for tok in sent] for sent in source_sents]\n",
        "  source_words_train = pad_sequences(source_words[:8*unit],padding='post')\n",
        "  source_words_dev = pad_sequences(source_words[8*unit:9*unit],padding='post')\n",
        "  source_words_test = pad_sequences(source_words[9*unit:],padding='post')\n",
        "\n",
        "  eos = target_lang_dict.word2ids['<end>']\n",
        "\n",
        "  target_words = [[target_lang_dict.word2ids.get(tok,target_lang_dict.UNK) for tok in sent[:-1]] for sent in target_sents]\n",
        "  target_words_train = pad_sequences(target_words[:8*unit],padding='post')\n",
        "\n",
        "  target_words_train_labels = [sent[1:]+[eos] for sent in target_words[:8*unit]]\n",
        "  target_words_train_labels = pad_sequences(target_words_train_labels,padding='post')\n",
        "  target_words_train_labels = np.expand_dims(target_words_train_labels,axis=2)\n",
        "\n",
        "  target_words_dev_labels = pad_sequences([sent[1:] + [eos] for sent in target_words[8 * unit:9 * unit]], padding='post')\n",
        "  target_words_test_labels = pad_sequences([sent[1:] + [eos] for sent in target_words[9 * unit:]], padding='post')\n",
        "\n",
        "  train_data = [source_words_train,target_words_train,target_words_train_labels]\n",
        "  dev_data = [source_words_dev,target_words_dev_labels]\n",
        "  test_data = [source_words_test,target_words_test_labels]\n",
        "\n",
        "  return train_data,dev_data,test_data,source_lang_dict,target_lang_dict\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  max_example = 30000\n",
        "  use_attention = False\n",
        "  train_data, dev_data, test_data, source_dict, target_dict = load_dataset(\"data.30.vi\",\"data.30.en\",max_num_examples=max_example)\n",
        "  print(\"read %d/%d/%d train/dev/test batches\" % (len(train_data[0]),len(dev_data[0]), len(test_data[0])))\n",
        "\n",
        "  model = NmtModel(source_dict,target_dict,use_attention)\n",
        "  model.build()\n",
        "  model.train(train_data,dev_data,test_data,10)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 24000/3000/3000 train/dev/test batches\n",
            "source vocab: 2034, target vocab:2506\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_7 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 100)    203400      input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 100)    250600      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, None, 100)    0           embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, None, 100)    0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 200),  240800      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, None, 200),  240800      dropout_4[0][0]                  \n",
            "                                                                 lstm_3[0][1]                     \n",
            "                                                                 lstm_3[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 2506)   503706      lstm_4[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,439,306\n",
            "Trainable params: 1,439,306\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, None)              0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, None, 100)         203400    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                [(None, None, 200), (None 240800    \n",
            "=================================================================\n",
            "Total params: 444,200\n",
            "Trainable params: 444,200\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_4 (Embedding)         (None, None, 100)    250600      input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, None, 100)    0           embedding_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, 200)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            (None, 200)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, None, 200),  240800      dropout_4[0][0]                  \n",
            "                                                                 input_8[0][0]                    \n",
            "                                                                 input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, None, 2506)   503706      lstm_4[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 995,106\n",
            "Trainable params: 995,106\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Starting training epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 38s 2ms/step - loss: 2.1322 - accuracy: 0.2425\n",
            "Time used for epoch 1: 0 m 39 s\n",
            "Evaluating on dev set after epoch 1/10:\n",
            "Model BLEU score: 1.65\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: and i &apos;m going to <unk> <unk> , and i &apos;m going to <unk> <unk> , and i &apos;m going to <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: and i &apos;m going to <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: the <unk> of the <unk> of <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and i &apos;m going to <unk> <unk> , and i &apos;m going to <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: the <unk> <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 2/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.8390 - accuracy: 0.2981\n",
            "Time used for epoch 2: 0 m 36 s\n",
            "Evaluating on dev set after epoch 2/10:\n",
            "Model BLEU score: 2.43\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: <unk> <unk> : <unk> <unk> , and it &apos;s a <unk> <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but it &apos;s not <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: i &apos;m going to be <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and it &apos;s a <unk> <unk> , and it &apos;s a <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: i &apos;m going to do that .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 3/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 36s 2ms/step - loss: 1.7316 - accuracy: 0.3254\n",
            "Time used for epoch 3: 0 m 36 s\n",
            "Evaluating on dev set after epoch 3/10:\n",
            "Model BLEU score: 2.99\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: the <unk> of the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> <unk> <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but it &apos;s a <unk> <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: the <unk> <unk> <unk> <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and we &apos;re going to be <unk> , and we &apos;re going to be <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: it &apos;s a <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 4/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 37s 2ms/step - loss: 1.6548 - accuracy: 0.3445\n",
            "Time used for epoch 4: 0 m 36 s\n",
            "Evaluating on dev set after epoch 4/10:\n",
            "Model BLEU score: 3.55\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: there &apos;s a <unk> <unk> , and i &apos;m going to be <unk> , and i &apos;m going to be <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but it &apos;s not just the <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: the <unk> <unk> <unk> <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and you can &apos;t be <unk> , and it &apos;s not just the <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: it &apos;s a <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 5/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 37s 2ms/step - loss: 1.6004 - accuracy: 0.3555\n",
            "Time used for epoch 5: 0 m 36 s\n",
            "Evaluating on dev set after epoch 5/10:\n",
            "Model BLEU score: 3.78\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: there &apos;s a <unk> <unk> , and the <unk> <unk> , <unk> <unk> , <unk> <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but the <unk> is not <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: the <unk> <unk> <unk> <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and you can see it , you know , it &apos;s a <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: the <unk> is <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 6/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 36s 1ms/step - loss: 1.5572 - accuracy: 0.3644\n",
            "Time used for epoch 6: 0 m 35 s\n",
            "Evaluating on dev set after epoch 6/10:\n",
            "Model BLEU score: 4.06\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: there &apos;s a <unk> <unk> , and it &apos;s a <unk> <unk> , and it &apos;s a <unk> <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but that &apos;s what &apos;s going on .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: this is a <unk> <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and you can see that <unk> <unk> <unk> <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: it &apos;s not a <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 7/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 36s 1ms/step - loss: 1.5239 - accuracy: 0.3701\n",
            "Time used for epoch 7: 0 m 35 s\n",
            "Evaluating on dev set after epoch 7/10:\n",
            "Model BLEU score: 4.13\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: there &apos;s a <unk> <unk> , and it &apos;s a <unk> <unk> , and it &apos;s <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but the <unk> is <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: they &apos;re <unk> by <unk> <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and you can see , <unk> <unk> <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: the <unk> is <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 8/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 36s 1ms/step - loss: 1.4973 - accuracy: 0.3740\n",
            "Time used for epoch 8: 0 m 35 s\n",
            "Evaluating on dev set after epoch 8/10:\n",
            "Model BLEU score: 4.14\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: there are a <unk> <unk> , and it &apos;s <unk> , and it &apos;s <unk> , and it &apos;s <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but that &apos;s what &apos;s happening .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: the <unk> <unk> is <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and you can see , the <unk> is <unk> <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: the <unk> <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 9/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 36s 1ms/step - loss: 1.4722 - accuracy: 0.3780\n",
            "Time used for epoch 9: 0 m 35 s\n",
            "Evaluating on dev set after epoch 9/10:\n",
            "Model BLEU score: 4.18\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: there &apos;s a <unk> <unk> , and he was <unk> , and it &apos;s a <unk> <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but this is a <unk> <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: this is a <unk> <unk> <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and you can see the <unk> <unk> , and it &apos;s <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: so it &apos;s <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 4 s\n",
            "Starting training epoch 10/10\n",
            "Epoch 1/1\n",
            "24000/24000 [==============================] - 35s 1ms/step - loss: 1.4534 - accuracy: 0.3813\n",
            "Time used for epoch 10: 0 m 35 s\n",
            "Evaluating on dev set after epoch 10/10:\n",
            "Model BLEU score: 4.55\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: there &apos;s a <unk> <unk> , and it &apos;s a <unk> <unk> , and it &apos;s <unk> , and it &apos;s <unk> .\n",
            "Actual Sentence: there are four <unk> <unk> that , each time this ring <unk> it , as it <unk> the <unk> of the display , it <unk> up a position signal .\n",
            "2 Sample\n",
            "Predicted Sentence: but this is the <unk> of the <unk> .\n",
            "Actual Sentence: but this is really just the beginning .\n",
            "3 Sample\n",
            "Predicted Sentence: these are <unk> <unk> <unk> .\n",
            "Actual Sentence: it <unk> this by <unk> <unk> about two <unk> .\n",
            "4 Sample\n",
            "Predicted Sentence: and you can see , you can see the <unk> <unk> .\n",
            "Actual Sentence: so as you can see here , this is a , <unk> <unk> <unk> board .\n",
            "5 Sample\n",
            "Predicted Sentence: and it &apos;s <unk> .\n",
            "Actual Sentence: these blocks <unk> <unk> .\n",
            "========================\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "Training finished!\n",
            "Time used for training: 6 m 44 s\n",
            "Evaluating on test set:\n",
            "Model BLEU score: 4.72\n",
            "===== PRINTING OUTPUT====\n",
            "1 Sample\n",
            "Predicted Sentence: the <unk> of the <unk> <unk> of <unk> <unk> , and he was <unk> .\n",
            "Actual Sentence: the second quote is from the head of the u.k. financial services <unk> .\n",
            "2 Sample\n",
            "Predicted Sentence: and it &apos;s <unk> .\n",
            "Actual Sentence: it gets worse .\n",
            "3 Sample\n",
            "Predicted Sentence: what does this mean ? what is the <unk> of the <unk> ?\n",
            "Actual Sentence: what &apos;s happening here ? how can this be possible ?\n",
            "4 Sample\n",
            "Predicted Sentence: it &apos;s not a <unk> of what &apos;s going on .\n",
            "Actual Sentence: unfortunately , the answer is yes .\n",
            "5 Sample\n",
            "Predicted Sentence: but , in fact , it &apos;s not a <unk> of <unk> , and it &apos;s not a <unk> of <unk> .\n",
            "Actual Sentence: but there &apos;s an <unk> solution which is coming from what is known as the science of <unk> .\n",
            "========================\n",
            "Time used for evaluate on test set: 0 m 3 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}