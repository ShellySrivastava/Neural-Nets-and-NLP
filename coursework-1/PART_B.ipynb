{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab4_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hExKCzh6doIW"
      },
      "source": [
        "# Lab 4 - Neural Network Classifier Using Simple Word Embeddings\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HixoFOoCIJ7V"
      },
      "source": [
        "In this session, we demonstrate how to solve a text classification task using simple \n",
        "feedforward neural network classifier. We will use IMDB Large Movie Review Dataset to train a binary classification model, able to predict whether a review is positive or negative. First, our network takes one-hot word vectors as input, averages them to make one vector and trains a \n",
        "fully-connected layer to predict the output. In the second part, we replace the one-hot vectors with the word embeddings and add a layer to see how much that improves the performance.\n",
        "\n",
        "We are going to use Keras Sequential API in this session. The Sequential API allows you to make models layer-by-layer. But it is not straightforward to define models where layers connect to more than just the previous and next layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m8fpBfhBpupy",
        "outputId": "35cb6d8d-f92b-457c-fe3b-43b2b4e95df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cqvPQvgvPv1W"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EundMtGPpCdf"
      },
      "source": [
        "The dataset we will be using is the IMDB Large Movie Review Dataset, which consists of 50000 labeled movie reviews. These are split into 25,000 reviews for training and 25,000 reviews for testing. The  dataset contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. The data is preprocessed. For text classification, it is ususal to limit the size of the vocabulary to stop the dataset from becoming too sparse, creating possible overfitting. We keep the top 10,000 most frequently occurring words in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NyuSzkafqNca",
        "outputId": "4d73289e-384c-4649-98e8-896444db9278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6U4iCV9-rmay"
      },
      "source": [
        "We now can start playing around with the data, letâ€™s first see the length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h-gjWRAuqg5s",
        "outputId": "11a2fb79-592b-491c-ebfc-db4453e0a621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(\"Training entries: {}, labels: {}\".format(len(X_train), len(y_train)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training entries: 25000, labels: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MTRZrpcyr-4x"
      },
      "source": [
        "The  reviews have been converted to integers and each integer represents a  word in a dictionary. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "79Ev72Kgq4XL",
        "outputId": "d3ff56af-c370-4ff9-c12e-e484bf2ee2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " X_train[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tvuu4KhStqei"
      },
      "source": [
        "We can convert integers back to words by querying a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gMCH1OoDrSNR",
        "outputId": "d07c1aac-bb46-47bf-985a-91564e6e6bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "word_index = imdb.get_word_index()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5IreFXgruZot"
      },
      "source": [
        "Index 1 represents the beginning of the sentence and the index 2 is assigned to all unknown tokens. Index 0 will be used for padding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "abIb7Fe5u3GQ",
        "colab": {}
      },
      "source": [
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  \n",
        "word_index[\"<UNUSED>\"] = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9TnnSuspvC5b"
      },
      "source": [
        "To reverse key and values in a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nKOiVVXQu-_I",
        "colab": {}
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZmTJEm8xvUvW"
      },
      "source": [
        "To view a word:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SqN5jgVKvJJZ",
        "outputId": "879b5e90-fe79-441a-e646-44455b6e6447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "reverse_word_index[25]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'film'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q6QjrzgVvrYn"
      },
      "source": [
        "And to recreate the whole sentence from our training data we define decode_review:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wvrKeMgxvWlv",
        "colab": {}
      },
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sxg4YA_NvdRg",
        "outputId": "7bafd62c-9dd5-4a46-fd88-5809fc644c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "decode_review(X_train[10])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> york seems went about two scenes the ? lunatic little ? anything next the now was people characters why this co ? <UNK> killing <UNK> movie and ? here <UNK> <UNK> ? lunatic like party for ahead lost ? josh little because turned seems warned ? a pass have the ? never york seems characters show him <UNK> i stories excellent movie ? use men for in a him more that to bit there team were have one ? freeman the ? with ? in to seemed a <UNK> one ? special totally ? life it jealousy ? with a ? recall ? many alice movie think worth outside company ? war ex ? government watch a ? your surprised <UNK> aspects and company that faded are a place to ? sleeping was wasn't human ? rude wood <UNK> started element <UNK> unwilling ? not usa fascinating ambitions could ? used these to ? sleeping job at scheming entertainment ? hoped set to ? system it slowly all though <UNK> could <UNK> going the is at genre derek most aka pop ? derek at between his side and enjoy is and potential go ? company hype of of war ex a ? good isn't bbc ? through new lot red it has silly there characters evil movie is this least an well understandable that down and bad was is or utterly and man it <UNK> funny ? never the or <UNK> this example or <UNK> ? with being new lady cons ? attempts and obsession can despite nothing strength <UNK> ground ? people imagine that ? with ? horror it because the is said set co ? government company sleeping <UNK> it ? with lady good coup ? in 3rd talky ? government star the ? proceeds jump and enjoy ex and potential just the ? sleeping or good named and much guys then or getting american it ? vhs a ? doesn't it which <UNK> do ? in with americans it this ? nothing <UNK> it me see being which his worst the seriously if a anime ? patient ? in race and jealousy it <UNK> new know vs and his lot walked ? with a let's that ? part the inside cop ? government many ? movies on he good isn't to story give made ? ? horror it ? goes definitely role said movie most give watch a good can interesting played superhero it ? with etc mouth one ? does etc his modern 3 this ? book long funny money ? attempts and his lot chess entertaining <UNK> a ? full people seems with ? have the ? never the ? keaton decent chemistry peter\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c8gIzXncfaJK"
      },
      "source": [
        "### Creating One-hot word vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B9W4yb3rv_E0"
      },
      "source": [
        "It is  common to use one-hot representation as input in Natural Language Processing tasks. In Keras, the Embedding layer takes an index as an input and convert it to one-hot vector with the length of the vocabulary size. Then multiplies these vectors by a normal weight matrix. But there is no way to only get a one-hot vector as the output of a layer in Keras. To solve this we use Lambda() layer and a function that creates the one-hot layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RPO_pK9zH4C5",
        "colab": {}
      },
      "source": [
        "def OneHot(input_dim=None, input_length=None):\n",
        "    \n",
        "    if input_dim is None or input_length is None:\n",
        "        raise TypeError(\"input_dim or input_length is not set\")\n",
        "\n",
        "    \n",
        "    def _one_hot(x, num_classes):\n",
        "        return K.one_hot(K.cast(x, 'uint8'),\n",
        "                          num_classes=num_classes)\n",
        "\n",
        "    return Lambda(_one_hot,\n",
        "                  arguments={'num_classes': input_dim},\n",
        "                  input_shape=(input_length,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "364d3MAw0ez9"
      },
      "source": [
        "input_dim refers to the length of the one-hot vector and input_length refers to the length of the input sequence. Since the input to K.one_hot should be an integer tensor, we cast x to one (Keras passes around float tensors by default).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VHz76GNA2M4r"
      },
      "source": [
        " Each text sequence has in most cases different length of words. Here, we fill sequences with a pad token (0) to fit the size. This special tokens is then masked not to be accounted in averaging, loss calculation etc. We set the maximum length to 256."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9G_o7PsvgSFt"
      },
      "source": [
        "### Preparing input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jiFn7sd_wF5j",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "X_train_enc = keras.preprocessing.sequence.pad_sequences(X_train,\n",
        "                                                        value=word_index[\"<PAD>\"],\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=256)\n",
        "\n",
        "X_test_enc = keras.preprocessing.sequence.pad_sequences(X_test,\n",
        "                                                       value=word_index[\"<PAD>\"],\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=256)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kcjFH1wKF_7d"
      },
      "source": [
        "And to view a padded review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zwH4dcfW_a18",
        "outputId": "b163563d-4973-4b1a-9794-2cb400d7b488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "print(X_train_enc[1])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
            "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
            "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
            "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
            "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
            "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
            "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
            "    7 8255    2  349 2637  148  605    2 8003   15  123  125   68    2\n",
            " 6853   15  349  165 4362   98    5    4  228    9   43    2 1157   15\n",
            "  299  120    5  120  174   11  220  175  136   50    9 4373  228 8255\n",
            "    5    2  656  245 2350    5    4 9837  131  152  491   18    2   32\n",
            " 7464 1212   14    9    6  371   78   22  625   64 1382    9    8  168\n",
            "  145   23    4 1690   15   16    4 1355    5   28    6   52  154  462\n",
            "   33   89   78  285   16  145   95    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1zcxFwNGepA"
      },
      "source": [
        "Now we want to build the neural network model. We  are going to have a hidden layer with 16 hidden units. \n",
        "\n",
        "First, we want to transform each index to an embedded vector and then average all vectors to a single one. It has been showed that unweighted average of word vectors outperforms many complicated networks that model semantic and syntactic compositionality. As an example you can take a look at this: (http://anthology.aclweb.org/P/P15/P15-1162.pdf)\n",
        "\n",
        "To average we need to ignore padded zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yi04MLIvJOGZ",
        "colab": {}
      },
      "source": [
        "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
        "    def call(self, x, mask=None):\n",
        "        if mask != None:\n",
        "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
        "        else:\n",
        "            return super().call(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "whgIIB5ggjna"
      },
      "source": [
        "### Neural Network model using one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jlOLnlnSJgrU"
      },
      "source": [
        "The first layer is an one-hot layer. The second layer is to compute average on all word vectors in a sentence without considering padding. The  output vector is piped through a fully-connected layer. The last layer is connected with a single output node with the sigmoid activation function. The final value is a float between 0 and 1. \n",
        "The vocabulary count of the movie reviews (10000) is used as the input shape. At the end we visualize the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Pn83gBbxiK7",
        "outputId": "f7389873-a2c8-4e6f-89f5-b3d6e6e6608f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(OneHot(VOCAB_SIZE, MAX_SEQUENCE_LENGTH))\n",
        "model.add(GlobalAveragePooling1DMasked())\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9h9j5ryXP4m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "e9bd2dc1-f2c1-4664-c6a2-bc29dbac6eb5"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_1 (Lambda)            (None, 256, 10000)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                160016    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3W5yY6zXXtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "751cba98-333b-457b-b725-84385c37588e"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model, show_shapes=True, show_layer_names=True, dpi = 70).create(prog='dot', format='svg'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"376pt\" viewBox=\"0.00 0.00 638.00 387.00\" width=\"620pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9722 .9722) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-383 634,-383 634,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140419470221096 -->\n<g class=\"node\" id=\"node1\">\n<title>140419470221096</title>\n<polygon fill=\"none\" points=\"151.5,-332.5 151.5,-378.5 478.5,-378.5 478.5,-332.5 151.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242.5\" y=\"-351.8\">lambda_1_input: InputLayer</text>\n<polyline fill=\"none\" points=\"333.5,-332.5 333.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"333.5,-355.5 391.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"362.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"391.5,-332.5 391.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"435\" y=\"-363.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"391.5,-355.5 478.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"435\" y=\"-340.3\">(None, 256)</text>\n</g>\n<!-- 140419470221040 -->\n<g class=\"node\" id=\"node2\">\n<title>140419470221040</title>\n<polygon fill=\"none\" points=\"156,-249.5 156,-295.5 474,-295.5 474,-249.5 156,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-268.8\">lambda_1: Lambda</text>\n<polyline fill=\"none\" points=\"284,-249.5 284,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"284,-272.5 342,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"313\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"342,-249.5 342,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"408\" y=\"-280.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"342,-272.5 474,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"408\" y=\"-257.3\">(None, 256, 10000)</text>\n</g>\n<!-- 140419470221096&#45;&gt;140419470221040 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140419470221096-&gt;140419470221040</title>\n<path d=\"M315,-332.3799C315,-324.1745 315,-314.7679 315,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"318.5001,-305.784 315,-295.784 311.5001,-305.784 318.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419470221152 -->\n<g class=\"node\" id=\"node3\">\n<title>140419470221152</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 630,-212.5 630,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-185.8\">global_average_pooling1d_masked_1: GlobalAveragePooling1DMasked</text>\n<polyline fill=\"none\" points=\"440,-166.5 440,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"440,-189.5 498,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"498,-166.5 498,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-197.3\">(None, 256, 10000)</text>\n<polyline fill=\"none\" points=\"498,-189.5 630,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"564\" y=\"-174.3\">(None, 10000)</text>\n</g>\n<!-- 140419470221040&#45;&gt;140419470221152 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140419470221040-&gt;140419470221152</title>\n<path d=\"M315,-249.3799C315,-241.1745 315,-231.7679 315,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"318.5001,-222.784 315,-212.784 311.5001,-222.784 318.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419470220928 -->\n<g class=\"node\" id=\"node4\">\n<title>140419470220928</title>\n<polygon fill=\"none\" points=\"181.5,-83.5 181.5,-129.5 448.5,-129.5 448.5,-83.5 181.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-102.8\">dense_1: Dense</text>\n<polyline fill=\"none\" points=\"288.5,-83.5 288.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"317.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"288.5,-106.5 346.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"317.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"346.5,-83.5 346.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-114.3\">(None, 10000)</text>\n<polyline fill=\"none\" points=\"346.5,-106.5 448.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-91.3\">(None, 16)</text>\n</g>\n<!-- 140419470221152&#45;&gt;140419470220928 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140419470221152-&gt;140419470220928</title>\n<path d=\"M315,-166.3799C315,-158.1745 315,-148.7679 315,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"318.5001,-139.784 315,-129.784 311.5001,-139.784 318.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419330020800 -->\n<g class=\"node\" id=\"node5\">\n<title>140419330020800</title>\n<polygon fill=\"none\" points=\"192.5,-.5 192.5,-46.5 437.5,-46.5 437.5,-.5 192.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"246\" y=\"-19.8\">dense_2: Dense</text>\n<polyline fill=\"none\" points=\"299.5,-.5 299.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"328.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"299.5,-23.5 357.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"328.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"357.5,-.5 357.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-31.3\">(None, 16)</text>\n<polyline fill=\"none\" points=\"357.5,-23.5 437.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 140419470220928&#45;&gt;140419330020800 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140419470220928-&gt;140419330020800</title>\n<path d=\"M315,-83.3799C315,-75.1745 315,-65.7679 315,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"318.5001,-56.784 315,-46.784 311.5001,-56.784 318.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Mz96xpCgvTj"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F3HbW_IKLqwT"
      },
      "source": [
        "To compile the model we need a loss function and an optimizer. We use binary_crossentropy loss function which is just a special case of categorical cross entropy. We also use Adam optimizer that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. You can read more about it here:\n",
        "(https://arxiv.org/abs/1412.6980v8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qh1PWTNMxjUw",
        "outputId": "d876dcf1-6c91-4cf2-f6d2-764179d34b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1jwQQqCN5Ia"
      },
      "source": [
        "When training, we want to check the accuracy of the model on data it hasn't seen before. So we create a validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f5lAqzQlxjSM",
        "colab": {}
      },
      "source": [
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "y_val = np.array(y_train[:10000])\n",
        "partial_y_train = np.array(y_train[10000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E8Kpo5G3OJEY"
      },
      "source": [
        "Then we start to train the model for 40 epochs in mini-batches of 512 samples and monitor the model's loss and accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "99_z39KAxjPi",
        "outputId": "ae119357-9836-485b-de44-3592aec9664d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "15000/15000 [==============================] - 12s 822us/step - loss: 0.6927 - acc: 0.5170 - val_loss: 0.6921 - val_acc: 0.5054\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6910 - acc: 0.5235 - val_loss: 0.6902 - val_acc: 0.6324\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.6889 - acc: 0.6399 - val_loss: 0.6882 - val_acc: 0.5581\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6862 - acc: 0.6343 - val_loss: 0.6854 - val_acc: 0.6413\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6831 - acc: 0.6531 - val_loss: 0.6822 - val_acc: 0.6533\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6794 - acc: 0.6588 - val_loss: 0.6784 - val_acc: 0.6656\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6753 - acc: 0.6768 - val_loss: 0.6745 - val_acc: 0.6682\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6706 - acc: 0.6791 - val_loss: 0.6699 - val_acc: 0.6717\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6658 - acc: 0.6769 - val_loss: 0.6651 - val_acc: 0.6748\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.6607 - acc: 0.6878 - val_loss: 0.6607 - val_acc: 0.6770\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6554 - acc: 0.6903 - val_loss: 0.6553 - val_acc: 0.6825\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.6502 - acc: 0.6930 - val_loss: 0.6503 - val_acc: 0.6891\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.6448 - acc: 0.6939 - val_loss: 0.6452 - val_acc: 0.6924\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6392 - acc: 0.6962 - val_loss: 0.6401 - val_acc: 0.6929\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.6340 - acc: 0.7008 - val_loss: 0.6350 - val_acc: 0.6911\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6286 - acc: 0.7013 - val_loss: 0.6305 - val_acc: 0.6969\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6233 - acc: 0.7061 - val_loss: 0.6254 - val_acc: 0.7032\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6182 - acc: 0.7061 - val_loss: 0.6208 - val_acc: 0.7028\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6132 - acc: 0.7089 - val_loss: 0.6155 - val_acc: 0.7040\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.6081 - acc: 0.7128 - val_loss: 0.6109 - val_acc: 0.7054\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.6033 - acc: 0.7159 - val_loss: 0.6066 - val_acc: 0.7117\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.5986 - acc: 0.7178 - val_loss: 0.6031 - val_acc: 0.7114\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.5941 - acc: 0.7177 - val_loss: 0.5978 - val_acc: 0.7136\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.5898 - acc: 0.7215 - val_loss: 0.5940 - val_acc: 0.7200\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.5856 - acc: 0.7222 - val_loss: 0.5903 - val_acc: 0.7202\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.5819 - acc: 0.7243 - val_loss: 0.5869 - val_acc: 0.7198\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.5778 - acc: 0.7267 - val_loss: 0.5829 - val_acc: 0.7237\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.5739 - acc: 0.7293 - val_loss: 0.5791 - val_acc: 0.7217\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.5704 - acc: 0.7301 - val_loss: 0.5755 - val_acc: 0.7252\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 3s 198us/step - loss: 0.5669 - acc: 0.7319 - val_loss: 0.5726 - val_acc: 0.7288\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.5640 - acc: 0.7326 - val_loss: 0.5692 - val_acc: 0.7273\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.5605 - acc: 0.7350 - val_loss: 0.5665 - val_acc: 0.7297\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.5576 - acc: 0.7385 - val_loss: 0.5634 - val_acc: 0.7304\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 3s 199us/step - loss: 0.5547 - acc: 0.7394 - val_loss: 0.5608 - val_acc: 0.7303\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.5523 - acc: 0.7401 - val_loss: 0.5583 - val_acc: 0.7313\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.5500 - acc: 0.7393 - val_loss: 0.5558 - val_acc: 0.7348\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 3s 195us/step - loss: 0.5471 - acc: 0.7415 - val_loss: 0.5536 - val_acc: 0.7383\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.5450 - acc: 0.7423 - val_loss: 0.5515 - val_acc: 0.7379\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 3s 197us/step - loss: 0.5426 - acc: 0.7436 - val_loss: 0.5496 - val_acc: 0.7380\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 3s 196us/step - loss: 0.5406 - acc: 0.7435 - val_loss: 0.5479 - val_acc: 0.7394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i_9a_rybhG5J"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EYLH8kOgOo9W"
      },
      "source": [
        "To evaulate the model on test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFMt2Q7b3taP",
        "outputId": "7e797527-acea-4bd7-c9e1-0229e1427ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "results = model.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 4s 144us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9RrKiPHcAmQU",
        "outputId": "bc02e08d-46ca-4d68-cad8-7e2c2e7ae256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(results)\n",
        "# loss, accuracay "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.5473038294219971, 0.73852]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pW7IpHxMO6qp"
      },
      "source": [
        "Our first model accuracy using one-hot vectors is \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwZk_yoWhPJB"
      },
      "source": [
        "### Plotting the accuracy graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JIDPH1J7PMzN"
      },
      "source": [
        "To plot a graph of accuracy and loss over time we can use Matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LS9k2vvSAqB7",
        "outputId": "1debf42f-c38a-47f9-e522-6c70878a1dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIkkpQIiISFIKECIikpQChIiIJPX/AWswHxasTQ7KAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a7OwOQw4h8RX"
      },
      "source": [
        "### Neural Network model using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l-QzOMO_P4jc"
      },
      "source": [
        "Now instead of one-hot vectors, we want to use embedding. We change our first layer in model1 to an Embedding layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: (batch, sequence, embedding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFrCsL-NBFVL",
        "outputId": "537a531d-8686-4b37-803c-30f6f8567fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "VOCAB_SIZE= 10000\n",
        "\n",
        "# put the code here\n",
        "model2 = Sequential()\n",
        "EMBED_SIZE = 100\n",
        "model2.add(Embedding(VOCAB_SIZE, EMBED_SIZE,input_length = MAX_SEQUENCE_LENGTH ))\n",
        "model2.add(GlobalAveragePooling1DMasked())\n",
        "model2.add(Dense(16, activation=\"relu\"))\n",
        "model2.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.summary()\n",
        "\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history2 = model2.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model2.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 256, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 16)                1616      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,001,633\n",
            "Trainable params: 1,001,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 1s 34us/step - loss: 0.6892 - acc: 0.5762 - val_loss: 0.6825 - val_acc: 0.6682\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.6697 - acc: 0.6877 - val_loss: 0.6528 - val_acc: 0.6894\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.6235 - acc: 0.7565 - val_loss: 0.5952 - val_acc: 0.7657\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.5490 - acc: 0.8183 - val_loss: 0.5187 - val_acc: 0.8243\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.4634 - acc: 0.8548 - val_loss: 0.4439 - val_acc: 0.8430\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.3881 - acc: 0.8747 - val_loss: 0.3886 - val_acc: 0.8590\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.3321 - acc: 0.8883 - val_loss: 0.3515 - val_acc: 0.8675\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.2915 - acc: 0.8991 - val_loss: 0.3274 - val_acc: 0.8734\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.2600 - acc: 0.9099 - val_loss: 0.3117 - val_acc: 0.8788\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.2367 - acc: 0.9167 - val_loss: 0.3007 - val_acc: 0.8808\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 0s 18us/step - loss: 0.2145 - acc: 0.9273 - val_loss: 0.2930 - val_acc: 0.8835\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.1971 - acc: 0.9335 - val_loss: 0.2896 - val_acc: 0.8827\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.1832 - acc: 0.9383 - val_loss: 0.2859 - val_acc: 0.8854\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.1687 - acc: 0.9451 - val_loss: 0.2848 - val_acc: 0.8848\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.1561 - acc: 0.9523 - val_loss: 0.2847 - val_acc: 0.8859\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.1445 - acc: 0.9571 - val_loss: 0.2862 - val_acc: 0.8858\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.1349 - acc: 0.9610 - val_loss: 0.2885 - val_acc: 0.8856\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 0s 18us/step - loss: 0.1255 - acc: 0.9639 - val_loss: 0.2917 - val_acc: 0.8850\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.1175 - acc: 0.9672 - val_loss: 0.2978 - val_acc: 0.8848\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.1107 - acc: 0.9694 - val_loss: 0.2999 - val_acc: 0.8833\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.1038 - acc: 0.9724 - val_loss: 0.3044 - val_acc: 0.8831\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0959 - acc: 0.9761 - val_loss: 0.3105 - val_acc: 0.8817\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0896 - acc: 0.9782 - val_loss: 0.3163 - val_acc: 0.8798\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.0839 - acc: 0.9805 - val_loss: 0.3224 - val_acc: 0.8811\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0789 - acc: 0.9822 - val_loss: 0.3284 - val_acc: 0.8790\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0739 - acc: 0.9845 - val_loss: 0.3369 - val_acc: 0.8770\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 0s 18us/step - loss: 0.0693 - acc: 0.9862 - val_loss: 0.3426 - val_acc: 0.8757\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0652 - acc: 0.9877 - val_loss: 0.3551 - val_acc: 0.8742\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0626 - acc: 0.9875 - val_loss: 0.3564 - val_acc: 0.8770\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0581 - acc: 0.9889 - val_loss: 0.3663 - val_acc: 0.8744\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.0546 - acc: 0.9901 - val_loss: 0.3738 - val_acc: 0.8728\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 0s 18us/step - loss: 0.0509 - acc: 0.9917 - val_loss: 0.3837 - val_acc: 0.8727\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0486 - acc: 0.9924 - val_loss: 0.3860 - val_acc: 0.8745\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.0456 - acc: 0.9928 - val_loss: 0.3992 - val_acc: 0.8713\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.0429 - acc: 0.9938 - val_loss: 0.4060 - val_acc: 0.8723\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.0401 - acc: 0.9944 - val_loss: 0.4110 - val_acc: 0.8729\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 0s 18us/step - loss: 0.0375 - acc: 0.9946 - val_loss: 0.4219 - val_acc: 0.8717\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0353 - acc: 0.9951 - val_loss: 0.4274 - val_acc: 0.8724\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 0s 17us/step - loss: 0.0333 - acc: 0.9961 - val_loss: 0.4378 - val_acc: 0.8711\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 0s 16us/step - loss: 0.0314 - acc: 0.9958 - val_loss: 0.4415 - val_acc: 0.8712\n",
            "25000/25000 [==============================] - 1s 36us/step\n",
            "[0.469260712428093, 0.8578]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "I4zIPJDcTPq3",
        "outputId": "505626a4-dfb2-49d6-cb2c-27866ecac941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "results = model2.evaluate(X_test_enc, y_test)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 35us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "waS96edDTRyL",
        "outputId": "f051b00a-a521-4295-e926-d409eab92c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print (results)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.469260712428093, 0.8578]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uShgAGuZYTnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "b6067bfc-8975-47cd-c7d4-8e80af5ad0db"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model2, show_shapes=True, show_layer_names=True, dpi = 70).create(prog='dot', format='svg'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"376pt\" viewBox=\"0.00 0.00 623.00 387.00\" width=\"606pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9722 .9722) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-383 619,-383 619,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140419470761936 -->\n<g class=\"node\" id=\"node1\">\n<title>140419470761936</title>\n<polygon fill=\"none\" points=\"132.5,-332.5 132.5,-378.5 482.5,-378.5 482.5,-332.5 132.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-351.8\">embedding_1_input: InputLayer</text>\n<polyline fill=\"none\" points=\"337.5,-332.5 337.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"337.5,-355.5 395.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"395.5,-332.5 395.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"439\" y=\"-363.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"395.5,-355.5 482.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"439\" y=\"-340.3\">(None, 256)</text>\n</g>\n<!-- 140419470758464 -->\n<g class=\"node\" id=\"node2\">\n<title>140419470758464</title>\n<polygon fill=\"none\" points=\"134.5,-249.5 134.5,-295.5 480.5,-295.5 480.5,-249.5 134.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-268.8\">embedding_1: Embedding</text>\n<polyline fill=\"none\" points=\"305.5,-249.5 305.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"305.5,-272.5 363.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"363.5,-249.5 363.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"422\" y=\"-280.3\">(None, 256)</text>\n<polyline fill=\"none\" points=\"363.5,-272.5 480.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"422\" y=\"-257.3\">(None, 256, 100)</text>\n</g>\n<!-- 140419470761936&#45;&gt;140419470758464 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140419470761936-&gt;140419470758464</title>\n<path d=\"M307.5,-332.3799C307.5,-324.1745 307.5,-314.7679 307.5,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"311.0001,-305.784 307.5,-295.784 304.0001,-305.784 311.0001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419470761600 -->\n<g class=\"node\" id=\"node3\">\n<title>140419470761600</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 615,-212.5 615,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-185.8\">global_average_pooling1d_masked_2: GlobalAveragePooling1DMasked</text>\n<polyline fill=\"none\" points=\"440,-166.5 440,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"440,-189.5 498,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"498,-166.5 498,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556.5\" y=\"-197.3\">(None, 256, 100)</text>\n<polyline fill=\"none\" points=\"498,-189.5 615,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"556.5\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 140419470758464&#45;&gt;140419470761600 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140419470758464-&gt;140419470761600</title>\n<path d=\"M307.5,-249.3799C307.5,-241.1745 307.5,-231.7679 307.5,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"311.0001,-222.784 307.5,-212.784 304.0001,-222.784 311.0001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419470760424 -->\n<g class=\"node\" id=\"node4\">\n<title>140419470760424</title>\n<polygon fill=\"none\" points=\"181.5,-83.5 181.5,-129.5 433.5,-129.5 433.5,-83.5 181.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-102.8\">dense_3: Dense</text>\n<polyline fill=\"none\" points=\"288.5,-83.5 288.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"317.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"288.5,-106.5 346.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"317.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"346.5,-83.5 346.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-114.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"346.5,-106.5 433.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-91.3\">(None, 16)</text>\n</g>\n<!-- 140419470761600&#45;&gt;140419470760424 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140419470761600-&gt;140419470760424</title>\n<path d=\"M307.5,-166.3799C307.5,-158.1745 307.5,-148.7679 307.5,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"311.0001,-139.784 307.5,-129.784 304.0001,-139.784 311.0001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419444584800 -->\n<g class=\"node\" id=\"node5\">\n<title>140419444584800</title>\n<polygon fill=\"none\" points=\"185,-.5 185,-46.5 430,-46.5 430,-.5 185,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-19.8\">dense_4: Dense</text>\n<polyline fill=\"none\" points=\"292,-.5 292,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"321\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"292,-23.5 350,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"321\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"350,-.5 350,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-31.3\">(None, 16)</text>\n<polyline fill=\"none\" points=\"350,-23.5 430,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"390\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 140419470760424&#45;&gt;140419444584800 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140419470760424-&gt;140419444584800</title>\n<path d=\"M307.5,-83.3799C307.5,-75.1745 307.5,-65.7679 307.5,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"311.0001,-56.784 307.5,-46.784 304.0001,-56.784 311.0001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XB7aveVzTC5a",
        "outputId": "6635da87-d420-4cf2-bdc3-c84078937267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history2.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNqRSHLpNAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FBpTc_rXGvQ"
      },
      "source": [
        "The accuracy of model2 is 87%. Using Embedding layer instead of one-hot layer improved the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "--020hfG6rN2"
      },
      "source": [
        "### Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J4gBeOyi4gkM"
      },
      "source": [
        "The Embedding layer can be used to load a pre-trained word embedding model. We are going to use GloVe embeddings, which you can read about it here (https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics. You can download GloVe and we can seed the Keras Embedding layer with weights from the pre-trained embedding for the words in your dataset.\n",
        "First, we need to read GloVe and map words to GloVe:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_PypdqG9Iis",
        "colab": {}
      },
      "source": [
        "def readGloveFile(gloveFile):\n",
        "    with open(gloveFile, 'r') as f:\n",
        "        wordToGlove = {}  \n",
        "        wordToIndex = {}  \n",
        "        indexToWord = {}  \n",
        "\n",
        "        for line in f:\n",
        "            record = line.strip().split()\n",
        "            token = record[0] \n",
        "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
        "            \n",
        "        tokens = sorted(wordToGlove.keys())\n",
        "        for idx, tok in enumerate(tokens):\n",
        "            kerasIdx = idx + 1  \n",
        "            wordToIndex[tok] = kerasIdx \n",
        "            indexToWord[kerasIdx] = tok \n",
        "\n",
        "    return wordToIndex, indexToWord, wordToGlove"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZcIZ3dq59bCh"
      },
      "source": [
        "Now, we create our pre-trained Embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gembn7VM3ex8",
        "colab": {}
      },
      "source": [
        "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
        "    vocabLen = len(wordToIndex) + 1  \n",
        "    embDim = next(iter(wordToGlove.values())).shape[0]  \n",
        "   \n",
        "    embeddingMatrix = np.zeros((vocabLen, embDim))  \n",
        "    for word, index in wordToIndex.items():\n",
        "        embeddingMatrix[index, :] = wordToGlove[word] \n",
        "\n",
        "    embeddingLayer = Embedding(vocabLen, embDim, embeddings_initializer=keras.initializers.Constant(embeddingMatrix), trainable=isTrainable)\n",
        "    return embeddingLayer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HGxciLK4-xOr"
      },
      "source": [
        "We freeze the weights. To create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PZCPUM0W_Drc",
        "outputId": "7ac3d60b-e957-47ba-aa1e-b8a18434e8f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# put the code here\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "wordToIndex, indexToWord, wordToGlove = readGloveFile('/content/drive/My Drive/glove.6B.300d.txt')\n",
        "\n",
        "print(len(wordToIndex))\n",
        "print(len(wordToGlove))\n",
        "print(len(indexToWord))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n",
            "400000\n",
            "400000\n",
            "400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M-bZ5SCHiIMl"
      },
      "source": [
        "### Adding another hidden layer to the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbZ6UBDfbjea"
      },
      "source": [
        "In model3, we only add another dense layer to see if that improves the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vw0le1YjDdCa",
        "outputId": "c753e0b8-c0a4-4a4f-b6b8-197488ef8dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# put your code here\n",
        "model3 = Sequential()\n",
        "model3.add(createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, True))\n",
        "model3.add(GlobalAveragePooling1DMasked())\n",
        "model3.add(Dense(16, activation=\"relu\"))\n",
        "model3.add(Dense(16, activation=\"relu\"))\n",
        "model3.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model3.summary()\n",
        "\n",
        "X_val = np.array(X_train_enc[:10000])\n",
        "partial_X_train = np.array(X_train_enc[10000:])\n",
        "\n",
        "history3 = model3.fit(partial_X_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    verbose=1)\n",
        "\n",
        "results = model3.evaluate(X_test_enc, y_test)\n",
        "print(results)\n",
        "results = model3.evaluate(X_test_enc, y_test)\n",
        "print(results)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 300)         120000300 \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_mas (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                4816      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 120,005,405\n",
            "Trainable params: 120,005,405\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:421: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120000300 elements. This may consume a large amount of memory.\n",
            "  num_elements)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 6s 433us/step - loss: 0.6922 - acc: 0.5231 - val_loss: 0.6856 - val_acc: 0.6043\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.6757 - acc: 0.6317 - val_loss: 0.6624 - val_acc: 0.6508\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.6368 - acc: 0.6971 - val_loss: 0.6068 - val_acc: 0.7292\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.5536 - acc: 0.7738 - val_loss: 0.5052 - val_acc: 0.7976\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.4421 - acc: 0.8322 - val_loss: 0.4155 - val_acc: 0.8365\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.3524 - acc: 0.8684 - val_loss: 0.3554 - val_acc: 0.8568\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.2946 - acc: 0.8889 - val_loss: 0.3251 - val_acc: 0.8686\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.2521 - acc: 0.9043 - val_loss: 0.3097 - val_acc: 0.8744\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.2201 - acc: 0.9183 - val_loss: 0.2965 - val_acc: 0.8773\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.1938 - acc: 0.9304 - val_loss: 0.2925 - val_acc: 0.8821\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.1706 - acc: 0.9413 - val_loss: 0.2944 - val_acc: 0.8823\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.1515 - acc: 0.9510 - val_loss: 0.2937 - val_acc: 0.8845\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.1344 - acc: 0.9577 - val_loss: 0.2998 - val_acc: 0.8837\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.1214 - acc: 0.9620 - val_loss: 0.3103 - val_acc: 0.8806\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 1s 98us/step - loss: 0.1111 - acc: 0.9654 - val_loss: 0.3202 - val_acc: 0.8805\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0965 - acc: 0.9717 - val_loss: 0.3256 - val_acc: 0.8797\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0863 - acc: 0.9764 - val_loss: 0.3368 - val_acc: 0.8787\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 1s 98us/step - loss: 0.0768 - acc: 0.9805 - val_loss: 0.3515 - val_acc: 0.8756\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0705 - acc: 0.9824 - val_loss: 0.3625 - val_acc: 0.8767\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0617 - acc: 0.9868 - val_loss: 0.3786 - val_acc: 0.8738\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 1s 98us/step - loss: 0.0552 - acc: 0.9893 - val_loss: 0.3960 - val_acc: 0.8715\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0499 - acc: 0.9910 - val_loss: 0.4098 - val_acc: 0.8705\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 1s 98us/step - loss: 0.0449 - acc: 0.9923 - val_loss: 0.4238 - val_acc: 0.8711\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0410 - acc: 0.9932 - val_loss: 0.4340 - val_acc: 0.8718\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0366 - acc: 0.9945 - val_loss: 0.4487 - val_acc: 0.8719\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0334 - acc: 0.9951 - val_loss: 0.4627 - val_acc: 0.8698\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0299 - acc: 0.9959 - val_loss: 0.4820 - val_acc: 0.8676\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0275 - acc: 0.9963 - val_loss: 0.5184 - val_acc: 0.8646\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0234 - acc: 0.9971 - val_loss: 0.5213 - val_acc: 0.8656\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0198 - acc: 0.9979 - val_loss: 0.5435 - val_acc: 0.8630\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0176 - acc: 0.9981 - val_loss: 0.5615 - val_acc: 0.8631\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 1s 98us/step - loss: 0.0160 - acc: 0.9984 - val_loss: 0.5750 - val_acc: 0.8631\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 1s 98us/step - loss: 0.0137 - acc: 0.9988 - val_loss: 0.5901 - val_acc: 0.8619\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0127 - acc: 0.9988 - val_loss: 0.6068 - val_acc: 0.8606\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0110 - acc: 0.9991 - val_loss: 0.6176 - val_acc: 0.8613\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0103 - acc: 0.9991 - val_loss: 0.6337 - val_acc: 0.8599\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0088 - acc: 0.9993 - val_loss: 0.6460 - val_acc: 0.8608\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0074 - acc: 0.9994 - val_loss: 0.6620 - val_acc: 0.8606\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 1s 96us/step - loss: 0.0067 - acc: 0.9996 - val_loss: 0.6830 - val_acc: 0.8592\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 1s 97us/step - loss: 0.0058 - acc: 0.9996 - val_loss: 0.6957 - val_acc: 0.8594\n",
            "25000/25000 [==============================] - 1s 39us/step\n",
            "[0.7407108402895928, 0.84988]\n",
            "25000/25000 [==============================] - 1s 37us/step\n",
            "[0.7407108402895928, 0.84988]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVvj3_ZPZIp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "6498372d-48e5-4e67-fc29-fb035ec558d9"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "from keras.utils import vis_utils\n",
        "SVG(vis_utils.model_to_dot(model3, show_shapes=True, show_layer_names=True, dpi = 70).create(prog='dot', format='svg'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"457pt\" viewBox=\"0.00 0.00 632.00 470.00\" width=\"614pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(.9722 .9722) rotate(0) translate(4 466)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-466 628,-466 628,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140419362994216 -->\n<g class=\"node\" id=\"node1\">\n<title>140419362994216</title>\n<polygon fill=\"none\" points=\"132.5,-415.5 132.5,-461.5 491.5,-461.5 491.5,-415.5 132.5,-415.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"235\" y=\"-434.8\">embedding_2_input: InputLayer</text>\n<polyline fill=\"none\" points=\"337.5,-415.5 337.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.5\" y=\"-446.3\">input:</text>\n<polyline fill=\"none\" points=\"337.5,-438.5 395.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"366.5\" y=\"-423.3\">output:</text>\n<polyline fill=\"none\" points=\"395.5,-415.5 395.5,-461.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"443.5\" y=\"-446.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"395.5,-438.5 491.5,-438.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"443.5\" y=\"-423.3\">(None, None)</text>\n</g>\n<!-- 140419362994832 -->\n<g class=\"node\" id=\"node2\">\n<title>140419362994832</title>\n<polygon fill=\"none\" points=\"134.5,-332.5 134.5,-378.5 489.5,-378.5 489.5,-332.5 134.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-351.8\">embedding_2: Embedding</text>\n<polyline fill=\"none\" points=\"305.5,-332.5 305.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"305.5,-355.5 363.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"334.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"363.5,-332.5 363.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426.5\" y=\"-363.3\">(None, None)</text>\n<polyline fill=\"none\" points=\"363.5,-355.5 489.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"426.5\" y=\"-340.3\">(None, None, 300)</text>\n</g>\n<!-- 140419362994216&#45;&gt;140419362994832 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140419362994216-&gt;140419362994832</title>\n<path d=\"M312,-415.3799C312,-407.1745 312,-397.7679 312,-388.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"315.5001,-388.784 312,-378.784 308.5001,-388.784 315.5001,-388.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419363439392 -->\n<g class=\"node\" id=\"node3\">\n<title>140419363439392</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 624,-295.5 624,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"220\" y=\"-268.8\">global_average_pooling1d_masked_3: GlobalAveragePooling1DMasked</text>\n<polyline fill=\"none\" points=\"440,-249.5 440,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"440,-272.5 498,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"498,-249.5 498,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561\" y=\"-280.3\">(None, None, 300)</text>\n<polyline fill=\"none\" points=\"498,-272.5 624,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"561\" y=\"-257.3\">(None, 300)</text>\n</g>\n<!-- 140419362994832&#45;&gt;140419363439392 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140419362994832-&gt;140419363439392</title>\n<path d=\"M312,-332.3799C312,-324.1745 312,-314.7679 312,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"315.5001,-305.784 312,-295.784 308.5001,-305.784 315.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419362994608 -->\n<g class=\"node\" id=\"node4\">\n<title>140419362994608</title>\n<polygon fill=\"none\" points=\"186,-166.5 186,-212.5 438,-212.5 438,-166.5 186,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"239.5\" y=\"-185.8\">dense_5: Dense</text>\n<polyline fill=\"none\" points=\"293,-166.5 293,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"293,-189.5 351,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"351,-166.5 351,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394.5\" y=\"-197.3\">(None, 300)</text>\n<polyline fill=\"none\" points=\"351,-189.5 438,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394.5\" y=\"-174.3\">(None, 16)</text>\n</g>\n<!-- 140419363439392&#45;&gt;140419362994608 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140419363439392-&gt;140419362994608</title>\n<path d=\"M312,-249.3799C312,-241.1745 312,-231.7679 312,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"315.5001,-222.784 312,-212.784 308.5001,-222.784 315.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419362993712 -->\n<g class=\"node\" id=\"node5\">\n<title>140419362993712</title>\n<polygon fill=\"none\" points=\"189.5,-83.5 189.5,-129.5 434.5,-129.5 434.5,-83.5 189.5,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243\" y=\"-102.8\">dense_6: Dense</text>\n<polyline fill=\"none\" points=\"296.5,-83.5 296.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325.5\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"296.5,-106.5 354.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325.5\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"354.5,-83.5 354.5,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394.5\" y=\"-114.3\">(None, 16)</text>\n<polyline fill=\"none\" points=\"354.5,-106.5 434.5,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394.5\" y=\"-91.3\">(None, 16)</text>\n</g>\n<!-- 140419362994608&#45;&gt;140419362993712 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140419362994608-&gt;140419362993712</title>\n<path d=\"M312,-166.3799C312,-158.1745 312,-148.7679 312,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"315.5001,-139.784 312,-129.784 308.5001,-139.784 315.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140419363455776 -->\n<g class=\"node\" id=\"node6\">\n<title>140419363455776</title>\n<polygon fill=\"none\" points=\"189.5,-.5 189.5,-46.5 434.5,-46.5 434.5,-.5 189.5,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243\" y=\"-19.8\">dense_7: Dense</text>\n<polyline fill=\"none\" points=\"296.5,-.5 296.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325.5\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"296.5,-23.5 354.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"325.5\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"354.5,-.5 354.5,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394.5\" y=\"-31.3\">(None, 16)</text>\n<polyline fill=\"none\" points=\"354.5,-23.5 434.5,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"394.5\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 140419362993712&#45;&gt;140419363455776 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140419362993712-&gt;140419363455776</title>\n<path d=\"M312,-83.3799C312,-75.1745 312,-65.7679 312,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"315.5001,-56.784 312,-46.784 308.5001,-56.784 315.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QtsdVeW7UgCu",
        "outputId": "1b88eb78-6d7e-4bbd-b162-873623b3dec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict = history3.history\n",
        "\n",
        "acc = history_dict['acc']\n",
        "val_acc = history_dict['val_acc']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYiIFDglAhGRAvf/AfRlLsJbAjk0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx--Ytk3ZbLo"
      },
      "source": [
        "The accuracy of model3 with an additional layer is 85%. Adding more layers can help you to extract more features. But we can do that upto a certain extent. After some point, instead of extracting features, we tend to overfit the data. Overfitting can lead to errors in some or the other form like false positives. It is not easy to choose the number of units in a hidden layer or the number of hidden layers in a neural network. For many applications, one hidden layer is enough. As a general rule, the number of units in that hidden layer is between the number of inputs and the number of outputs.\n",
        " The best way to decide on the number of units and hidden layers is to try various parameters. Train several neural networks with different numbers of hidden layers and neurons, and monitor the performance of them. You will have to experiment using a series of different architectures. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2xCgorWUs1",
        "colab_type": "text"
      },
      "source": [
        "***Adding another dense layer doesn't in increasing the accuracy of the model. Adding more dense layers can actually start to overfit the model. In these cases, a dropput layer should be added to prevent overfitting. The dropout layer freezes the weights of a certain layer so that it is not update during back propagation. ***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gn2GSV4ioyO2"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XYC6DykEox2w",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GsCJ01StlgCx"
      },
      "source": [
        "This tutorial is substantially based on this document:\n",
        "https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
        "\n",
        "To read more about Sequential APIs you can go to: https://keras.io/getting-started/sequential-model-guide/\n",
        "\n",
        "The one-hot word vector layer is taken from:\n",
        "https://fdalvi.github.io/blog/2018-04-07-keras-sequential-onehot/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jL0UovfaE9GE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}